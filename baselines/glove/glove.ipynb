{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kumarmohit/opt/anaconda3/envs/cil_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIRECTORY=\"glove.twitter.27B/\"\n",
    "GLOVE_FILE=\"glove.twitter.27B.200d.txt\"\n",
    "input_path = 'twitter-datasets/' \n",
    "ouput_path = 'output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the files and converting each word to embedding corresponding to the glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = list(open(input_path + \"train_pos.txt\", \"r\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]  # -1000\n",
    "negative_examples = list(open(input_path + \"train_neg.txt\", \"r\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]\n",
    "x = positive_examples + negative_examples\n",
    "positive_labels = [1 for _ in positive_examples]\n",
    "negative_labels = [0 for _ in negative_examples]\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "X_train= np.array(x)[shuffle_indices]\n",
    "Y_train = np.array(y)[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading glove embeddings from directory\n",
    "embeddings = {}\n",
    "with open(os.path.join(GLOVE_DIRECTORY, GLOVE_FILE), 'r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 180000\n",
      "# Test data samples: 20000\n",
      "# Train data samples: 180000\n",
      "# Test data samples: 20000\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', x_train.shape[0])\n",
    "print('# Test data samples:', x_test.shape[0])\n",
    "print('# Train data samples:', y_train.shape[0])\n",
    "print('# Test data samples:', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i) for i in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hahahaha subtwitters make laugh ! stupid dumbass !'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging the embedding for each sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_train_embeddings = []\n",
    "for line in x_train:\n",
    "    line_emb = np.zeros(200)\n",
    "    fline_emb=[]\n",
    "    length = len(line)\n",
    "    for word in line.strip().split():\n",
    "        tmp=[]\n",
    "        if word in embeddings.keys():\n",
    "            for (item1, item2) in zip(line_emb, embeddings[word]):\n",
    "                tmp.append(item1+item2)\n",
    "            line_emb=tmp        \n",
    "    for item in line_emb:\n",
    "        fline_emb.append(item/length)\n",
    "    line_train_embeddings.append(fline_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ouput_path + \"train_embeddings.npy\",line_train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_test_embeddings = []\n",
    "for line in x_test:\n",
    "    line_emb = np.zeros(200)\n",
    "    fline_emb=[]\n",
    "    length = len(line)\n",
    "    for word in line.strip().split():\n",
    "        tmp=[]\n",
    "        if word in embeddings.keys():\n",
    "            for (item1, item2) in zip(line_emb, embeddings[word]):\n",
    "                tmp.append(item1+item2)\n",
    "            line_emb=tmp        \n",
    "    for item in line_emb:\n",
    "        fline_emb.append(item/length)\n",
    "    line_test_embeddings.append(fline_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ouput_path + \"test_embeddings.npy\",line_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line_test_embeddings[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=8, penalty='l2',max_iter=500)\n",
    "model=scikit_log_reg.fit(line_train_embeddings,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78125\n"
     ]
    }
   ],
   "source": [
    "score = model.score(line_test_embeddings, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-5, random_state=42, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-05, max_iter=100, random_state=42)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(line_train_embeddings,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7813\n"
     ]
    }
   ],
   "source": [
    "score = model.score(line_test_embeddings, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000,max_depth=None,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(line_train_embeddings,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(line_test_embeddings, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_examples = list(open(input_path + \"test_data.txt\", \"r\").readlines())\n",
    "final_test_embeddings = []\n",
    "for line in final_examples:\n",
    "    line_emb = np.zeros(200)\n",
    "    fline_emb=[]\n",
    "    length = len(line)\n",
    "    for word in line.strip().split():\n",
    "        tmp=[]\n",
    "        if word in embeddings.keys():\n",
    "            for (item1, item2) in zip(line_emb, embeddings[word]):\n",
    "                tmp.append(item1+item2)\n",
    "            line_emb=tmp        \n",
    "    for item in line_emb:\n",
    "        fline_emb.append(item/length)\n",
    "    final_test_embeddings.append(fline_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ouput_path + \"final_test_embeddings.npy\",final_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_test_embeddings[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=model.predict(final_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=[]\n",
    "for index, row in enumerate(sub):\n",
    "    if(row==0):\n",
    "        ans.append(-1)\n",
    "    else:\n",
    "        ans.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def replace_com_tab(file1):\n",
    "    idd=[]\n",
    "    val=[]\n",
    "    text_pos = open(file1, \"r\")\n",
    "    for line in text_pos:\n",
    "        currentline = line.split(\",\",1)\n",
    "        #print(currentline[1])\n",
    "        idd.append(int(currentline[0]))\n",
    "        #tmp=currentline[1].replace(r'\\n', ' ', regex=True)\n",
    "        val.append(currentline[1])\n",
    "    data={1 : val}\n",
    "    #print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df1= pd.DataFrame({\n",
    "    'id':idd,\n",
    "    'text': df[1].replace(r'\\n', ' ', regex=True)\n",
    "    })\n",
    "    #return df\n",
    "    #df1.to_csv('../data/test1.tsv', sep='\\t',index=False)\n",
    "    return idd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idd=replace_com_tab(input_path + 'test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav=pd.DataFrame({'Id':idd,'Prediction':ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav.to_csv(ouput_path + 'RF.csv',index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.77 score for logistic regression and 0.758 with random forest Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
